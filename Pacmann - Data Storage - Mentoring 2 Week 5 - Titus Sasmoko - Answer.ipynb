{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f600792",
   "metadata": {},
   "source": [
    "# **Exercise 2 \\- Data Storage**\n",
    "\n",
    "## **Data Storage, Sekolah Engineering, Pacmann**\n",
    "\n",
    "**Outline**\n",
    "\n",
    "**[Objective 2](#objective)**\n",
    "\n",
    "[**Task Description 2**](#task-description)\n",
    "\n",
    "[Step \\#1 \\- Requirements Gathering (10 points) 2](#step-#1---requirements-gathering-\\(10-points\\))\n",
    "\n",
    "[Step \\#2 \\- Slowly Changing Dimension (SCD) (15 points) 2](#step-#2---slowly-changing-dimension-\\(scd\\)-\\(15-points\\))\n",
    "\n",
    "[Step \\#3 \\- ELT with Python & SQL (50 points) 3](#step-#3---elt-with-python-&-sql-\\(50-points\\))\n",
    "\n",
    "[Step \\#4 \\- Orchestrate ELT with Luigi (20 points) 3](#step-#4---orchestrate-elt-with-luigi-\\(20-points\\))\n",
    "\n",
    "[Step \\#5 \\- Create Report(5 points) 3](#step-#5---create-report\\(5-points\\))\n",
    "\n",
    "# **Objective**\n",
    "\n",
    "The objective of this exercise is to:\n",
    "\n",
    "- Implement Slowly Changing Dimensions (SCD)\n",
    "- Build an ELT process using Python and SQL\n",
    "- Orchestrate the ELT process using Luigi\n",
    "\n",
    "# **Task Description**\n",
    "\n",
    "In the first exercise, you designed a Data Warehouse model for Olist. The next step is to determine the historical data and versioning requirements the company wishes to maintain and building a data pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f1e3d2e",
   "metadata": {},
   "source": [
    "## **Step \\#1 \\- Requirements Gathering (10 points)**\n",
    "\n",
    "In this step, pretend to have a meeting with stakeholders to find out their needs for the SCD strategy you will implement. Make **a list of questions** to ask them. After each question, provide a **possible answe**r that a stakeholder might give based on the dataset and business needs. This information will help you create your SCD strategy.\n",
    "\n",
    "- **Create Questions:** Write questions to ask stakeholders about the Data Warehouse.\n",
    "- **Provide Stakeholder Answers:** Give a possible answer for each question based on Olist's business and dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5cf20a7",
   "metadata": {},
   "source": [
    "### **Answers**\n",
    "\n",
    "#### 1. Which dimension tables are likely to have changes overtime that we should track of?\n",
    "  \n",
    "  **Response**\n",
    "  \n",
    "  Primarily `customers` and `sellers`. </br>\n",
    "  Customers may change their address or city (captured via `geolocation`) and sellers sometimes update their location or business names. </br>\n",
    "  `products` and `category name` are mostly static, but we may occassionally reclassify a product category\n",
    "  \n",
    "#### 2. Do we need to maintain historical versions of data?\n",
    "  \n",
    "  **Response**\n",
    "  \n",
    "  We wanted to preserve historical changes for both `customers` and `sellers`, especially location changes, as they can affect delivery time and logistics. </br>\n",
    "  For `products` and `category_name`, just keeping the latest version is enough since changes are rare and typically administrative </br>\n",
    "  \n",
    "#### 3. What kind of historical tracking is preferred, -- full versioning (type 2), overwrite (type 1), or current vs previous field (type 3)?\n",
    "  \n",
    "  **Response**\n",
    "  \n",
    "  SCD Type 2 (full-versioning)\n",
    "  \n",
    "  - `dim_customers`\n",
    "  - `dim_sellers`\n",
    "  \n",
    "  SCD Type 1 (overwrite)\n",
    "  \n",
    "  - `dim_category_name`\n",
    "  - `dim_products`\n",
    "\n",
    "#### 4. How should we handle geolocation changes? Should customers and sellers be linked to geolocation with historical traceability?\n",
    "  \n",
    "  **Response**\n",
    "  \n",
    "  Yes, because geolocation information, current or previous, is valuable for analysis, eg. comparing delivery times or satisfaction scores before and after a move. </br>\n",
    "  So we'll need foreign key-based Type 2 handling for geolocation information.\n",
    "  \n",
    "#### 5. Who will consume the historical data, and how will it be used (eg. in reports, machine learning, fraud detection, etc)?\n",
    "  \n",
    "  **Response**\n",
    "  \n",
    "  - Data analyst and operations team\n",
    "    - delivery performance reports, churn analysis, and predictive models\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b11d13",
   "metadata": {},
   "source": [
    "## **Step \\#2 \\- Slowly Changing Dimension (SCD) (15 Points)**\n",
    "\n",
    "From the requirements you gathered, decide **which SCD strategies** you want to use. Explain why you chose these strategies.\n",
    "\n",
    "- You can implement more than one **SCD strategies**\n",
    "- If there are any changes to the existing Entity-Relationship Diagram (ERD), show the new ERD.\n",
    "\n",
    "(Note: You will earn extra points for using SCD types other than Type 1.)\n",
    "\n",
    "### **Answers**\n",
    "\n",
    "**SCD Type 1 / Overwrite**\n",
    "\n",
    "- `dim_product_category_name`\n",
    "    - track changes in the names of the category\n",
    "    - utilizing Type 1 / overwrite so as to simplify the overall process\n",
    "    - historical tracking of category names are not significant because at the end of the day, the main analysis to be conducted are mainly about products, customers, sellers, locations, and periodicals.\n",
    "\n",
    "**SCD Type 2 / Add New Row-Column**\n",
    "\n",
    "- `dim_customers`\n",
    "    - customers may change their location\n",
    "    - because of this change, logistic fees and performance changes may affect future purchases\n",
    "- `dim_sellers`\n",
    "    - sellers may change their location\n",
    "    - because of this change, \n",
    "- `dim_products`\n",
    "    - changes in products may reflect changes in purchases\n",
    "    - for example, the bigger the dimension may inflict higher freight cost, which may affect purchasing decision\n",
    "    - also, the changes in price tends to affect purchasing decision also, especially if the customers are price-sensitive\n",
    "    - hence, it is important to track price changes and dimension changes in relation to the total price & cost that the customers need to pay."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a778ed62",
   "metadata": {},
   "source": [
    "---\n",
    "## **Step \\#3 \\- ELT with Python & SQL (50 points)**\n",
    "\n",
    "Create a data pipeline to process ELT using Python and SQL.\n",
    "\n",
    "- Workflow Description: Draw your workflow and explain how it handles your SCD strategy.\n",
    "- Write clean code for your scripts\n",
    "- include alerts for any errors.\n",
    "\n",
    "### **Answers**\n",
    "\n",
    "Workflow\n",
    "  - Establish the tables in `stg` schema in `olist-dwh` database\n",
    "  - Establish the `dim` and `fact` tables in `dwh` schema in `olist-dwh` database\n",
    "  - Create a script to populate the `dim_date` and `dim_time` tables in `dwh` schema in `olist-dwh` database\n",
    "  - Establish ELT script to pull data from `olist-src` database to `stg` schema in `olist-dwh` database \n",
    "  - Establish ELT script to pull data from `stg` schema in `olist-dwh` database to `dwh` schema in `olist-dwh` database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "84889823",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sql extension is already loaded. To reload it, use:\n",
      "  %reload_ext sql\n"
     ]
    }
   ],
   "source": [
    "import sqlalchemy\n",
    "import psycopg2\n",
    "%load_ext sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "64f8d029",
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql postgresql://postgres:postgres@localhost:5434/olist-dwh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d5280a4",
   "metadata": {},
   "source": [
    "**Schema in source database (`olist-src`)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e594603e",
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "CREATE TABLE public.customers (\n",
    "    customer_id text NOT NULL,\n",
    "    customer_unique_id text,\n",
    "    customer_zip_code_prefix integer,\n",
    "    customer_city text,\n",
    "    customer_state text\n",
    ");\n",
    "\n",
    "CREATE TABLE public.geolocation (\n",
    "    geolocation_zip_code_prefix integer NOT NULL,\n",
    "    geolocation_lat real,\n",
    "    geolocation_lng real,\n",
    "    geolocation_city text,\n",
    "    geolocation_state text\n",
    ");\n",
    "\n",
    "CREATE TABLE public.order_items (\n",
    "    order_id text NOT NULL,\n",
    "    order_item_id integer NOT NULL,\n",
    "    product_id text,\n",
    "    seller_id text,\n",
    "    shipping_limit_date text,\n",
    "    price real,\n",
    "    freight_value real\n",
    ");\n",
    "\n",
    "CREATE TABLE public.order_payments (\n",
    "    order_id text NOT NULL,\n",
    "    payment_sequential integer NOT NULL,\n",
    "    payment_type text,\n",
    "    payment_installments integer,\n",
    "    payment_value real\n",
    ");\n",
    "\n",
    "CREATE TABLE public.order_reviews (\n",
    "    review_id text NOT NULL,\n",
    "    order_id text NOT NULL,\n",
    "    review_score integer,\n",
    "    review_comment_title text,\n",
    "    review_comment_message text,\n",
    "    review_creation_date text\n",
    ");\n",
    "\n",
    "CREATE TABLE public.orders (\n",
    "    order_id text NOT NULL,\n",
    "    customer_id text,\n",
    "    order_status text,\n",
    "    order_purchase_timestamp text,\n",
    "    order_approved_at text,\n",
    "    order_delivered_carrier_date text,\n",
    "    order_delivered_customer_date text,\n",
    "    order_estimated_delivery_date text\n",
    ");\n",
    "\n",
    "CREATE TABLE public.product_category_name_translation (\n",
    "    product_category_name text NOT NULL,\n",
    "    product_category_name_english text\n",
    ");\n",
    "\n",
    "CREATE TABLE public.products (\n",
    "    product_id text NOT NULL,\n",
    "    product_category_name text,\n",
    "    product_name_lenght real,\n",
    "    product_description_lenght real,\n",
    "    product_photos_qty real,\n",
    "    product_weight_g real,\n",
    "    product_length_cm real,\n",
    "    product_height_cm real,\n",
    "    product_width_cm real\n",
    ");\n",
    "\n",
    "CREATE TABLE public.sellers (\n",
    "    seller_id text NOT NULL,\n",
    "    seller_zip_code_prefix integer,\n",
    "    seller_city text,\n",
    "    seller_state text\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d523bb37",
   "metadata": {},
   "source": [
    "**Establish tables in the `public` schema in the `olist_dwh` database**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "3509e87f",
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * postgresql://postgres:***@localhost:5434/olist-dwh\n",
      "Done.\n",
      "Done.\n",
      "Done.\n",
      "Done.\n",
      "Done.\n",
      "Done.\n",
      "Done.\n",
      "Done.\n",
      "Done.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS public.geolocation (\n",
    "    geolocation_zip_code_prefix INTEGER UNIQUE NOT NULL,\n",
    "    geolocation_lat REAL,\n",
    "    geolocation_lng REAL,\n",
    "    geolocation_city TEXT,\n",
    "    geolocation_state TEXT\n",
    ");\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS public.product_category_name_translation (\n",
    "    product_category_name TEXT UNIQUE NOT NULL,\n",
    "    product_category_name_english TEXT\n",
    ");\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS public.sellers (\n",
    "    seller_id TEXT UNIQUE NOT NULL,\n",
    "    seller_zip_code_prefix INTEGER,\n",
    "    seller_city TEXT,\n",
    "    seller_state TEXT\n",
    ");\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS public.customers (\n",
    "    customer_id TEXT UNIQUE NOT NULL,\n",
    "    customer_unique_id TEXT,\n",
    "    customer_zip_code_prefix INTEGER,\n",
    "    customer_city TEXT,\n",
    "    customer_state TEXT\n",
    ");\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS public.products (\n",
    "    product_id TEXT UNIQUE NOT NULL,\n",
    "    product_category_name TEXT REFERENCES public.product_category_name_translation(product_category_name),\n",
    "    product_name_lenght REAL,\n",
    "    product_description_lenght REAL,\n",
    "    product_photos_qty REAL,\n",
    "    product_weight_g REAL,\n",
    "    product_length_cm REAL,\n",
    "    product_height_cm REAL,\n",
    "    product_width_cm REAL\n",
    ");\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS public.orders (\n",
    "    order_id TEXT UNIQUE NOT NULL,\n",
    "    customer_id TEXT REFERENCES public.customers(customer_id),\n",
    "    order_status TEXT,\n",
    "    order_purchase_timestamp TEXT,\n",
    "    order_approved_at TEXT,\n",
    "    order_delivered_carrier_date TEXT,\n",
    "    order_delivered_customer_date TEXT,\n",
    "    order_estimated_delivery_date TEXT\n",
    ");\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS public.order_items (\n",
    "    order_id TEXT NOT NULL REFERENCES public.orders(order_id),\n",
    "    order_item_id INTEGER NOT NULL,\n",
    "    product_id TEXT REFERENCES public.products(product_id),\n",
    "    seller_id TEXT REFERENCES public.sellers(seller_id),\n",
    "    shipping_limit_date TEXT,\n",
    "    price NUMERIC,\n",
    "    freight_value NUMERIC\n",
    ");\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS public.order_payments (\n",
    "    order_id TEXT NOT NULL,\n",
    "    payment_sequential INTEGER NOT NULL,\n",
    "    payment_type TEXT,\n",
    "    payment_installments INTEGER,\n",
    "    payment_value NUMERIC\n",
    ");\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS public.order_reviews (\n",
    "    review_id text NOT NULL,\n",
    "    order_id text NOT NULL REFERENCES public.orders(order_id),\n",
    "    review_score integer,\n",
    "    review_comment_title text,\n",
    "    review_comment_message text,\n",
    "    review_creation_date text\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe14fd7",
   "metadata": {},
   "source": [
    "**Establish tables in the `stg` schema in `olist-dwh` database**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "c3bfc680",
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * postgresql://postgres:***@localhost:5434/olist-dwh\n",
      "Done.\n",
      "Done.\n",
      "Done.\n",
      "Done.\n",
      "Done.\n",
      "Done.\n",
      "Done.\n",
      "Done.\n",
      "Done.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS stg.geolocation (\n",
    "    geolocation_zip_code_prefix INTEGER UNIQUE NOT NULL,\n",
    "    geolocation_lat REAL,\n",
    "    geolocation_lng REAL,\n",
    "    geolocation_city TEXT,\n",
    "    geolocation_state TEXT\n",
    ");\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS stg.product_category_name_translation (\n",
    "    product_category_name TEXT UNIQUE NOT NULL,\n",
    "    product_category_name_english TEXT\n",
    ");\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS stg.sellers (\n",
    "    seller_id TEXT UNIQUE NOT NULL,\n",
    "    seller_zip_code_prefix INTEGER,\n",
    "    seller_city TEXT,\n",
    "    seller_state TEXT\n",
    ");\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS stg.customers (\n",
    "    customer_id TEXT UNIQUE NOT NULL,\n",
    "    customer_unique_id TEXT,\n",
    "    customer_zip_code_prefix INTEGER,\n",
    "    customer_city TEXT,\n",
    "    customer_state TEXT\n",
    ");\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS stg.products (\n",
    "    product_id TEXT UNIQUE NOT NULL,\n",
    "    product_category_name TEXT REFERENCES stg.product_category_name_translation(product_category_name),\n",
    "    product_name_length REAL,\n",
    "    product_description_length REAL,\n",
    "    product_photos_qty REAL,\n",
    "    product_weight_g REAL,\n",
    "    product_length_cm REAL,\n",
    "    product_height_cm REAL,\n",
    "    product_width_cm REAL\n",
    ");\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS stg.orders (\n",
    "    order_id TEXT UNIQUE NOT NULL,\n",
    "    customer_id TEXT REFERENCES stg.customers(customer_id),\n",
    "    order_status TEXT,\n",
    "    order_purchase_timestamp TEXT,\n",
    "    order_approved_at TEXT,\n",
    "    order_delivered_carrier_date TEXT,\n",
    "    order_delivered_customer_date TEXT,\n",
    "    order_estimated_delivery_date TEXT\n",
    ");\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS stg.order_items (\n",
    "    order_id TEXT NOT NULL REFERENCES stg.orders(order_id),\n",
    "    order_item_id INTEGER NOT NULL,\n",
    "    product_id TEXT REFERENCES stg.products(product_id),\n",
    "    seller_id TEXT REFERENCES stg.sellers(seller_id),\n",
    "    shipping_limit_date TEXT,\n",
    "    price NUMERIC,\n",
    "    freight_value NUMERIC\n",
    ");\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS stg.order_payments (\n",
    "    order_id TEXT NOT NULL,\n",
    "    payment_sequential INTEGER NOT NULL,\n",
    "    payment_type TEXT,\n",
    "    payment_installments INTEGER,\n",
    "    payment_value NUMERIC\n",
    ");\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS stg.order_reviews (\n",
    "    review_id text UNIQUE NOT NULL,\n",
    "    order_id text NOT NULL REFERENCES stg.orders(order_id),\n",
    "    review_score integer,\n",
    "    review_comment_title text,\n",
    "    review_comment_message text,\n",
    "    review_creation_date text\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c981f6b2",
   "metadata": {},
   "source": [
    "**Establish `dim` and `facts` tables in the `dwh` schema in `olist-dwh` database**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "d3342d46",
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * postgresql://postgres:***@localhost:5434/olist-dwh\n",
      "Done.\n",
      "Done.\n",
      "Done.\n",
      "Done.\n",
      "Done.\n",
      "Done.\n",
      "Done.\n",
      "Done.\n",
      "Done.\n",
      "Done.\n",
      "Done.\n",
      "Done.\n",
      "Done.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "\n",
    "CREATE EXTENSION IF NOT EXISTS \"uuid-ossp\";\n",
    "\n",
    "CREATE SCHEMA IF NOT EXISTS dwh AUTHORIZATION postgres;\n",
    "\n",
    "CREATE TABLE dwh.dim_date (\n",
    "    date_id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),\n",
    "    full_date TIMESTAMP UNIQUE NOT NULL,\n",
    "    date_only DATE UNIQUE NOT NULL,\n",
    "    year NUMERIC NOT NULL,\n",
    "    month NUMERIC NOT NULL,\n",
    "    day NUMERIC NOT NULL,\n",
    "    week NUMERIC NOT NULL,\n",
    "    quarter NUMERIC NOT NULL,\n",
    "    semester NUMERIC NOT NULL,\n",
    "    day_of_week VARCHAR(15) NOT NULL,\n",
    "    month_name VARCHAR(15) NOT NULL,\n",
    "    year_day VARCHAR(15) UNIQUE NOT NULL, --eg. 2025-02\n",
    "    year_month VARCHAR(8) NOT NULL, -- e.g. '2025-06'\n",
    "    year_week VARCHAR(8) NOT NULL, -- e.g. '2025-W23'\n",
    "    year_quarter VARCHAR(8) NOT NULL, -- e.g. '2025-Q2'\n",
    "    year_semester VARCHAR(8) NOT NULL -- e.g. '2025-S1'\n",
    ");\n",
    "\n",
    "CREATE TABLE dwh.dim_time (\n",
    "    time_id TIME PRIMARY KEY,\n",
    "    hour SMALLINT NOT NULL,\n",
    "    minute SMALLINT NOT NULL,\n",
    "    second SMALLINT NOT NULL\n",
    ");\n",
    "\n",
    "CREATE TABLE dwh.dim_geolocation (\n",
    "    geolocation_sk SERIAL PRIMARY KEY,\n",
    "    geolocation_zip_code_prefix INTEGER UNIQUE NOT NULL,\n",
    "    geolocation_lat REAL,\n",
    "    geolocation_lng REAL,\n",
    "    geolocation_city TEXT,\n",
    "    geolocation_state TEXT\n",
    ");\n",
    "\n",
    "CREATE TABLE dwh.dim_product_category (\n",
    "    product_category_sk SERIAL PRIMARY KEY,\n",
    "    product_category_name TEXT UNIQUE NOT NULL,\n",
    "    product_category_name_english TEXT\n",
    ");\n",
    "\n",
    "CREATE TABLE dwh.dim_sellers (\n",
    "    seller_sk SERIAL PRIMARY KEY,\n",
    "    seller_id TEXT UNIQUE NOT NULL,\n",
    "    seller_zip_code_prefix INTEGER,\n",
    "    seller_city TEXT,\n",
    "    seller_state TEXT,\n",
    "    record_start_date DATE NOT NULL DEFAULT CURRENT_DATE,\n",
    "    record_end_date DATE,\n",
    "    is_current BOOLEAN NOT NULL DEFAULT TRUE\n",
    ");\n",
    "\n",
    "CREATE TABLE dwh.dim_customers (\n",
    "    customer_sk SERIAL PRIMARY KEY,\n",
    "    customer_id TEXT UNIQUE NOT NULL,\n",
    "    customer_unique_id TEXT,\n",
    "    customer_zip_code_prefix INTEGER,\n",
    "    customer_city TEXT,\n",
    "    customer_state TEXT,\n",
    "    record_start_date DATE NOT NULL DEFAULT CURRENT_DATE,\n",
    "    record_end_date DATE,\n",
    "    is_current BOOLEAN NOT NULL DEFAULT TRUE\n",
    ");\n",
    "\n",
    "CREATE TABLE dwh.dim_products (\n",
    "    product_sk SERIAL PRIMARY KEY,\n",
    "    product_id TEXT NOT NULL,\n",
    "    product_category_name TEXT REFERENCES dwh.dim_product_category(product_category_name),\n",
    "    product_name_length REAL,\n",
    "    product_description_length REAL,\n",
    "    product_photos_qty REAL,\n",
    "    product_weight_g REAL,\n",
    "    product_length_cm REAL,\n",
    "    product_height_cm REAL,\n",
    "    product_width_cm REAL,\n",
    "    record_start_date DATE NOT NULL DEFAULT CURRENT_DATE,\n",
    "    record_end_date DATE,\n",
    "    is_current BOOLEAN NOT NULL DEFAULT TRUE\n",
    ");\n",
    "\n",
    "\n",
    "CREATE TABLE dwh.fact_orders_comprehensive (\n",
    "    order_compr_id SERIAL PRIMARY KEY, -- surrogate key\n",
    "\n",
    "    order_id TEXT NOT NULL, -- natural key             \n",
    "    order_date_id DATE NOT NULL REFERENCES dwh.dim_date(full_date),\n",
    "    order_time_id TIME NOT NULL REFERENCES dwh.dim_time(time_id),\n",
    "    customer_sk INTEGER NOT NULL REFERENCES dwh.dim_customers(customer_sk),\n",
    "    seller_sk INTEGER NOT NULL REFERENCES dwh.dim_sellers(seller_sk),\n",
    "    product_sk INTEGER NOT NULL REFERENCES dwh.dim_products(product_sk),\n",
    "    product_category_sk INTEGER NOT NULL REFERENCES dwh.dim_product_category(product_category_sk),\n",
    "\n",
    "    price NUMERIC NOT NULL,\n",
    "    freight_value NUMERIC NOT NULL,\n",
    "    total_value NUMERIC GENERATED ALWAYS AS (price + freight_value) STORED,\n",
    "\n",
    "    inserted_at TIMESTAMP DEFAULT now()\n",
    ");\n",
    "\n",
    "CREATE TABLE dwh.fact_reviews_sentiment(\n",
    "  review_id TEXT NOT NULL,\n",
    "\n",
    "  order_compr_id INTEGER NOT NULL REFERENCES dwh.fact_orders_comprehensive(order_compr_id),\n",
    "  customer_sk INTEGER NOT NULL REFERENCES dwh.dim_customers(customer_sk),\n",
    "  product_sk INTEGER NOT NULL REFERENCES dwh.dim_products(product_sk),\n",
    "\n",
    "  review_title TEXT,\n",
    "  review_comment_message TEXT,\n",
    "  review_score INTEGER\n",
    ");\n",
    "\n",
    "CREATE TABLE dwh.daily_periodical_snapshot_order_trends (\n",
    "    daily_snapshot_id SERIAL PRIMARY KEY,\n",
    "    order_date DATE UNIQUE REFERENCES dwh.dim_date(date_only),\n",
    "    year_day VARCHAR NOT NULL,\n",
    "    year_week VARCHAR NOT NULL,\n",
    "    year_month VARCHAR NOT NULL,\n",
    "    year_quarter VARCHAR NOT NULL,\n",
    "    year_semester VARCHAR NOT NULL,\n",
    "    distinct_customer_count INTEGER NOT NULL,\n",
    "    distinct_seller_count INTEGER NOT NULL,\n",
    "    order_id_count INTEGER NOT NULL,\n",
    "    product_id_count INTEGER NOT NULL,\n",
    "    sum_price NUMERIC NOT NULL,\n",
    "    sum_freight_value NUMERIC NOT NULL,\n",
    "    inserted_at TIMESTAMP NOT NULL DEFAULT NOW()\n",
    ");\n",
    "\n",
    "CREATE TABLE dwh.accumulating_snapshot_logistic_performance (\n",
    "    accm_order_id SERIAL PRIMARY KEY,\n",
    "    order_id TEXT,\n",
    "    product_sk INTEGER NOT NULL REFERENCES dwh.dim_products(product_sk),\n",
    "    seller_sk INTEGER NOT NULL REFERENCES dwh.dim_sellers(seller_sk),\n",
    "    customer_sk INTEGER NOT NULL REFERENCES dwh.dim_customers(customer_sk),\n",
    "\n",
    "    order_purchase_timestamp TIMESTAMP NOT NULL,\n",
    "    order_approved_at TIMESTAMP,\n",
    "    purchase_approve_difference INTERVAL,\n",
    "\n",
    "    order_estimated_delivery_date TIMESTAMP,\n",
    "    order_shipping_limit_date TIMESTAMP,\n",
    "    order_delivered_carrier_date TIMESTAMP,\n",
    "    order_delivered_customer_date TIMESTAMP,\n",
    "\n",
    "    approved_carrier_difference INTERVAL,\n",
    "    approved_delivered_difference INTERVAL,\n",
    "\n",
    "    order_purchase_date_id UUID REFERENCES dwh.dim_date(date_id),\n",
    "    order_delivered_date_id UUID REFERENCES dwh.dim_date(date_id),\n",
    "\n",
    "    last_updated TIMESTAMP NOT NULL DEFAULT NOW(),\n",
    "\n",
    "    order_status_code VARCHAR\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00febdf",
   "metadata": {},
   "source": [
    "**Script to populate `dim_date` and `dim_time` in `dwh` schema in `olist-dwh` database**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "237bb0f2",
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "%%sql \n",
    "\n",
    "WITH date_series AS (\n",
    "    SELECT generate_series(DATE '2015-01-01', DATE '2020-12-31', INTERVAL '1 day')::DATE AS full_date\n",
    "),\n",
    "\n",
    "date_enriched AS (\n",
    "    SELECT\n",
    "        gen_random_uuid() AS date_id,\n",
    "        full_date,\n",
    "        full_date AS date_only,\n",
    "        EXTRACT(YEAR FROM full_date)::INT AS year,\n",
    "        EXTRACT(MONTH FROM full_date)::INT AS month,\n",
    "        EXTRACT(DAY FROM full_date)::INT AS day,\n",
    "        EXTRACT(WEEK FROM full_date)::INT AS week,\n",
    "        EXTRACT(QUARTER FROM full_date)::INT AS quarter,\n",
    "        CASE WHEN EXTRACT(MONTH FROM full_date) <= 6 THEN 1 ELSE 2 END AS semester,\n",
    "        TO_CHAR(full_date, 'Day') AS day_of_week,\n",
    "        TO_CHAR(full_date, 'Month') AS month_name,\n",
    "        TO_CHAR(full_date, 'YYYY-DDD') AS year_day,\n",
    "        TO_CHAR(full_date, 'YYYY-MM') AS year_month,\n",
    "        TO_CHAR(full_date, 'IYYY-IW') AS year_week,\n",
    "        TO_CHAR(full_date, 'YYYY-\"Q\"Q') AS year_quarter,\n",
    "        TO_CHAR(full_date, 'YYYY-\"S\"') || CASE WHEN EXTRACT(MONTH FROM full_date) <= 6 THEN '1' ELSE '2' END AS year_semester\n",
    "    FROM date_series\n",
    ")\n",
    "\n",
    "INSERT INTO dwh.dim_date (\n",
    "    date_id, full_date, date_only,\n",
    "    year, month, day, week, quarter, semester,\n",
    "    day_of_week, month_name, year_day, year_month, year_week, year_quarter, year_semester\n",
    ")\n",
    "SELECT\n",
    "    date_id, full_date, date_only,\n",
    "    year, month, day, week, quarter, semester,\n",
    "    TRIM(day_of_week), TRIM(month_name),\n",
    "    year_day, year_month, year_week, year_quarter, year_semester\n",
    "FROM date_enriched\n",
    "ORDER BY full_date;\n",
    "\n",
    "\n",
    "DO $$\n",
    "DECLARE\n",
    "    h INT;\n",
    "    m INT;\n",
    "    s INT;\n",
    "    time_val TIME;\n",
    "BEGIN\n",
    "    FOR h IN 0..23 LOOP\n",
    "        FOR m IN 0..59 LOOP\n",
    "            FOR s IN 0..59 LOOP\n",
    "                time_val := MAKE_TIME(h, m, s);\n",
    "                INSERT INTO dim_time (time_id, hour, minute, second)\n",
    "                VALUES (time_val, h, m, s);\n",
    "            END LOOP;\n",
    "        END LOOP;\n",
    "    END LOOP;\n",
    "END $$;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae45eb24",
   "metadata": {},
   "source": [
    "**ELT to populate `stg` schema in `olist-dwh` database from `public` schema in `olist-src` database`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff4ae50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transferring data for geolocation...\n",
      "Transferring data for product_category_name_translation...\n",
      "Transferring data for sellers...\n",
      "Transferring data for customers...\n",
      "Transferring data for products...\n",
      "Transferring data for orders...\n",
      "Transferring data for order_items...\n",
      "Transferring data for order_payments...\n",
      "Transferring data for order_reviews...\n",
      "Data transfer completed successfully!\n"
     ]
    }
   ],
   "source": [
    "# simple code\n",
    "import psycopg2\n",
    "\n",
    "# Connection details\n",
    "src_db_params = {\n",
    "    \"dbname\": \"olist-src\",\n",
    "    \"user\": \"postgres\",\n",
    "    \"password\": \"postgres\",\n",
    "    \"host\": \"localhost\",\n",
    "    \"port\": \"5433\"\n",
    "}\n",
    "\n",
    "dst_db_params = {\n",
    "    \"dbname\": \"olist-dwh\",\n",
    "    \"user\": \"postgres\",\n",
    "    \"password\": \"postgres\",\n",
    "    \"host\": \"localhost\",\n",
    "    \"port\": \"5434\"\n",
    "}\n",
    "\n",
    "# Tables to transfer\n",
    "tables = [\n",
    "    \"geolocation\", \"product_category_name_translation\", \n",
    "    \"sellers\", \"customers\", \"products\", \n",
    "    \"orders\", \"order_items\", \"order_payments\", \"order_reviews\"\n",
    "]\n",
    "\n",
    "try:\n",
    "    # Connect to source database\n",
    "    src_conn = psycopg2.connect(**src_db_params)\n",
    "    src_cursor = src_conn.cursor()\n",
    "\n",
    "    # Connect to destination database\n",
    "    dst_conn = psycopg2.connect(**dst_db_params)\n",
    "    dst_cursor = dst_conn.cursor()\n",
    "\n",
    "    for table in tables:\n",
    "        print(f\"Transferring data for {table}...\")\n",
    "\n",
    "        # Fetch data from the source table\n",
    "        src_cursor.execute(f\"SELECT * FROM public.{table}\")\n",
    "        rows = src_cursor.fetchall()\n",
    "\n",
    "        # Truncate destination table to prevent duplicates\n",
    "        dst_cursor.execute(f\"TRUNCATE TABLE stg.{table} RESTART IDENTITY CASCADE\")\n",
    "\n",
    "        # Insert extracted data into destination table\n",
    "        for row in rows:\n",
    "            placeholders = \", \".join([\"%s\"] * len(row))\n",
    "            query = f\"INSERT INTO stg.{table} VALUES ({placeholders})\"\n",
    "            dst_cursor.execute(query, row)\n",
    "\n",
    "    # Commit changes\n",
    "    dst_conn.commit()\n",
    "    print(\"Data transfer completed successfully!\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"Error:\", e)\n",
    "\n",
    "finally:\n",
    "    # Close connections\n",
    "    src_cursor.close()\n",
    "    src_conn.close()\n",
    "    dst_cursor.close()\n",
    "    dst_conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3523bf8",
   "metadata": {},
   "source": [
    "**ELT to populate `stg` schema from `public` schema in `olist-dwh` database**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ed132d",
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "INSERT INTO stg.geolocation\n",
    "    (geolocation_zip_code_prefix,\n",
    "    geolocation_lat,\n",
    "    geolocation_lng,\n",
    "    geolocation_city,\n",
    "    geolocation_state) \n",
    "SELECT\n",
    "    geolocation_zip_code_prefix, \n",
    "    geolocation_lat,\n",
    "    geolocation_lng,\n",
    "    geolocation_city,\n",
    "    geolocation_state\n",
    "FROM public.geolocation\n",
    "\n",
    "ON CONFLICT (geolocation_zip_code_prefix) DO NOTHING;\n",
    "\n",
    "INSERT INTO stg.product_category_name_translation \n",
    "    (product_category_name, \n",
    "    product_category_name_english) \n",
    "SELECT\n",
    "    product_category_name, \n",
    "    product_category_name_english\n",
    "FROM public.product_name_category_translation\n",
    "\n",
    "ON CONFLICT(product_category_name) DO NOTHING;\n",
    "\n",
    "INSERT INTO stg.sellers (\n",
    "  seller_id, seller_zip_code_prefix, seller_city, seller_state\n",
    ")\n",
    "SELECT\n",
    "  seller_id, seller_zip_code_prefix, seller_city, seller_state\n",
    "FROM public.sellers\n",
    "\n",
    "ON CONFLICT(seller_id) DO NOTHING;\n",
    "\n",
    "INSERT INTO stg.customers (\n",
    "  customer_id, customer_unique_id, customer_zip_code_prefix, customer_city, customer_state\n",
    ")\n",
    "SELECT customer_id, customer_unique_id, customer_zip_code_prefix, customer_city, customer_state\n",
    "FROM public.customers\n",
    "\n",
    "ON CONFLICT(customer_id) DO NOTHING;\n",
    "\n",
    "INSERT INTO stg.products (\n",
    "  product_id, product_category_name,\n",
    "  product_name_length, product_description_length,\n",
    "  product_photos_qty, product_weight_g,\n",
    "  product_length_cm, product_height_cm, product_width_cm\n",
    ")\n",
    "SELECT product_id, product_category_name,\n",
    "  product_name_length, product_description_length,\n",
    "  product_photos_qty, product_weight_g,\n",
    "  product_length_cm, product_height_cm, product_width_cm\n",
    "FROM public.products\n",
    "ON CONFLICT(product_id) DO NOTHING;\n",
    "\n",
    "INSERT INTO stg.orders (\n",
    "  order_id, customer_id,\n",
    "  order_status, order_purchase_timestamp, order_approved_at,\n",
    "  order_delivered_carrier_date, order_delivered_customer_date, order_estimated_delivery_date\n",
    ")\n",
    "SELECT order_id, customer_id,\n",
    "  order_status, order_purchase_timestamp, order_approved_at,\n",
    "  order_delivered_carrier_date, order_delivered_customer_date, order_estimated_delivery_date\n",
    "FROM public.orders\n",
    "ON CONFLICT(order_id) DO NOTHING;\n",
    "\n",
    "INSERT INTO stg.order_items (\n",
    "  order_id, order_item_id,\n",
    "  product_id, seller_id, \n",
    "  shipping_limit_date, price, freight_value\n",
    " )\n",
    "SELECT order_id, order_item_id,\n",
    "  product_id, seller_id, \n",
    "  shipping_limit_date, price, freight_value\n",
    "FROM public.order_items;\n",
    "\n",
    "INSERT INTO stg.order_payments (\n",
    "  order_id, payment_sequential, payment_type, payment_installments, payment_value\n",
    ")\n",
    "SELECT order_id, payment_sequential, payment_type, payment_installments, payment_value\n",
    "FROM public.order_payments;\n",
    "\n",
    "INSERT INTO stg.order_reviews (\n",
    "  review_id, order_id, \n",
    "  review_score, review_comment_title, review_comment_message, \n",
    "  review_creation_date\n",
    ")\n",
    "SELECT review_id, order_id, \n",
    "  review_score, review_comment_title, review_comment_message, \n",
    "  review_creation_date\n",
    "FROM public.order_reviews\n",
    "ON CONFLICT(review_id) DO NOTHING;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80552d8a",
   "metadata": {},
   "source": [
    "**ELT Script for `dim` tables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5452730e",
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (284399526.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[8], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    INSERT INTO dwh.dim_geolocation (\u001b[0m\n\u001b[1;37m           ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "\n",
    "-- dim geolocation\n",
    "INSERT INTO dwh.dim_geolocation (\n",
    "    geolocation_zip_code_prefix,\n",
    "    geolocation_lat, geolocation_lng,\n",
    "    geolocation_city, geolocation_state\n",
    ")\n",
    "SELECT geolocation_zip_code_prefix, \n",
    "  geolocation_lat, geolocation_lng, \n",
    "  geolocation_city, geolocation_state\n",
    "FROM stg.geolocation\n",
    "WHERE geolocation_zip_code_prefix IS NOT NULL\n",
    "ON CONFLICT (geolocation_zip_code_prefix) DO NOTHING;\n",
    "\n",
    "-- dim product category\n",
    "INSERT INTO dwh.dim_product_category (\n",
    "    product_category_name,\n",
    "    product_category_name_english\n",
    "    )\n",
    " SELECT DISTINCT\n",
    "    product_category_name,\n",
    "    product_category_name_english\n",
    " FROM stg.product_category_name_translation\n",
    " WHERE product_category_name IS NOT NULL\n",
    " ON CONFLICT(product_category_name) DO NOTHING;\n",
    "\n",
    "-- dim sellers\n",
    "-- Step 1: Expire old records if there's a change\n",
    "UPDATE dwh.dim_sellers d\n",
    "SET \n",
    "    record_end_date = CURRENT_DATE - INTERVAL '1 day',\n",
    "    is_current = FALSE\n",
    "FROM stg.sellers s\n",
    "WHERE d.seller_id = s.seller_id\n",
    "  AND d.is_current = TRUE\n",
    "  AND (\n",
    "        d.seller_zip_code_prefix IS DISTINCT FROM s.seller_zip_code_prefix OR\n",
    "        d.seller_city IS DISTINCT FROM s.seller_city OR\n",
    "        d.seller_state IS DISTINCT FROM s.seller_state\n",
    "      );\n",
    "\n",
    "-- Step 2: Insert new records (only new or changed)\n",
    "INSERT INTO dwh.dim_sellers (\n",
    "    seller_id,\n",
    "    seller_zip_code_prefix,\n",
    "    seller_city,\n",
    "    seller_state,\n",
    "    record_start_date,\n",
    "    is_current\n",
    ")\n",
    "SELECT\n",
    "    s.seller_id,\n",
    "    s.seller_zip_code_prefix,\n",
    "    s.seller_city,\n",
    "    s.seller_state,\n",
    "    CURRENT_DATE,\n",
    "    TRUE\n",
    "FROM stg.sellers s\n",
    "LEFT JOIN dwh.dim_sellers d\n",
    "  ON s.seller_id = d.seller_id AND d.is_current = TRUE\n",
    "WHERE \n",
    "    d.seller_id IS NULL -- new\n",
    "    OR (\n",
    "        d.seller_zip_code_prefix IS DISTINCT FROM s.seller_zip_code_prefix OR\n",
    "        d.seller_city IS DISTINCT FROM s.seller_city OR\n",
    "        d.seller_state IS DISTINCT FROM s.seller_state\n",
    "    );\n",
    "\n",
    "-- dim customers\n",
    "UPDATE dwh.dim_customers d\n",
    "SET \n",
    "    is_current = FALSE,\n",
    "    record_end_date = CURRENT_DATE\n",
    "FROM stg.customers c\n",
    "WHERE \n",
    "    d.customer_id = c.customer_id\n",
    "    AND d.is_current = TRUE\n",
    "    AND (\n",
    "        d.customer_unique_id IS DISTINCT FROM c.customer_unique_id OR\n",
    "        d.customer_zip_code_prefix IS DISTINCT FROM c.customer_zip_code_prefix OR\n",
    "        d.customer_city IS DISTINCT FROM c.customer_city OR\n",
    "        d.customer_state IS DISTINCT FROM c.customer_state\n",
    "    );\n",
    "\n",
    "INSERT INTO dwh.dim_customers (\n",
    "    customer_id,\n",
    "    customer_unique_id,\n",
    "    customer_zip_code_prefix,\n",
    "    customer_city,\n",
    "    customer_state,\n",
    "    record_start_date,\n",
    "    is_current\n",
    ")\n",
    "SELECT\n",
    "    c.customer_id,\n",
    "    c.customer_unique_id,\n",
    "    c.customer_zip_code_prefix,\n",
    "    c.customer_city,\n",
    "    c.customer_state,\n",
    "    CURRENT_DATE,\n",
    "    TRUE\n",
    "FROM stg.customers c\n",
    "LEFT JOIN dwh.dim_customers d\n",
    "    ON c.customer_id = d.customer_id AND d.is_current = TRUE\n",
    "WHERE \n",
    "    d.customer_id IS NULL OR (\n",
    "        d.customer_unique_id IS DISTINCT FROM c.customer_unique_id OR\n",
    "        d.customer_zip_code_prefix IS DISTINCT FROM c.customer_zip_code_prefix OR\n",
    "        d.customer_city IS DISTINCT FROM c.customer_city OR\n",
    "        d.customer_state IS DISTINCT FROM c.customer_state\n",
    "    );\n",
    "\n",
    "-- dim products\n",
    "-- Step 1: Expire the current records with changes\n",
    "UPDATE dwh.dim_products d\n",
    "SET \n",
    "    record_end_date = CURRENT_DATE - INTERVAL '1 day',\n",
    "    is_current = FALSE\n",
    "FROM stg.products p\n",
    "WHERE d.product_id = p.product_id\n",
    "  AND d.is_current = TRUE\n",
    "  AND (\n",
    "      d.product_category_name IS DISTINCT FROM p.product_category_name OR\n",
    "      d.product_name_length IS DISTINCT FROM p.product_name_length OR\n",
    "      d.product_description_length IS DISTINCT FROM p.product_description_length OR\n",
    "      d.product_photos_qty IS DISTINCT FROM p.product_photos_qty OR\n",
    "      d.product_weight_g IS DISTINCT FROM p.product_weight_g OR\n",
    "      d.product_length_cm IS DISTINCT FROM p.product_length_cm OR\n",
    "      d.product_height_cm IS DISTINCT FROM p.product_height_cm OR\n",
    "      d.product_width_cm IS DISTINCT FROM p.product_width_cm\n",
    "  );\n",
    "\n",
    "-- Step 2: Insert new records (new or changed)\n",
    "INSERT INTO dwh.dim_products (\n",
    "    product_id,\n",
    "    product_category_name,\n",
    "    product_name_length,\n",
    "    product_description_length,\n",
    "    product_photos_qty,\n",
    "    product_weight_g,\n",
    "    product_length_cm,\n",
    "    product_height_cm,\n",
    "    product_width_cm,\n",
    "    record_start_date,\n",
    "    is_current\n",
    ")\n",
    "SELECT DISTINCT\n",
    "    p.product_id,\n",
    "    p.product_category_name,\n",
    "    p.product_name_length,\n",
    "    p.product_description_length,\n",
    "    p.product_photos_qty,\n",
    "    p.product_weight_g,\n",
    "    p.product_length_cm,\n",
    "    p.product_height_cm,\n",
    "    p.product_width_cm,\n",
    "    CURRENT_DATE,\n",
    "    TRUE\n",
    "FROM stg.products p\n",
    "LEFT JOIN dwh.dim_products d\n",
    "  ON p.product_id = d.product_id AND d.is_current = TRUE\n",
    "WHERE \n",
    "    d.product_id IS NULL -- New product\n",
    "    OR (\n",
    "      d.product_category_name IS DISTINCT FROM p.product_category_name OR\n",
    "      d.product_name_length IS DISTINCT FROM p.product_name_length OR\n",
    "      d.product_description_length IS DISTINCT FROM p.product_description_length OR\n",
    "      d.product_photos_qty IS DISTINCT FROM p.product_photos_qty OR\n",
    "      d.product_weight_g IS DISTINCT FROM p.product_weight_g OR\n",
    "      d.product_length_cm IS DISTINCT FROM p.product_length_cm OR\n",
    "      d.product_height_cm IS DISTINCT FROM p.product_height_cm OR\n",
    "      d.product_width_cm IS DISTINCT FROM p.product_width_cm\n",
    "    );"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f8abbde",
   "metadata": {},
   "source": [
    "**ELT code for comprehensive `fact` tables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c6cee24",
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "-- fact order comprehensive\n",
    "INSERT INTO dwh.fact_orders_comprehensive (\n",
    "    order_id, order_date_id, order_time_id,\n",
    "    customer_sk, seller_sk, product_sk, product_category_sk,\n",
    "    price, freight_value, quantity\n",
    ")\n",
    "SELECT\n",
    "    oi.order_id, dd.full_date, dt.time_id,\n",
    "    dc.customer_sk, ds.seller_sk, dp.product_sk, dpc.product_category_sk,\n",
    "    oi.price, oi.freight_value, oi.quantity\n",
    "\n",
    "FROM stg.order_items oi\n",
    "JOIN stg.orders o ON oi.order_id = o.order_id\n",
    "\n",
    "JOIN dwh.dim_date dd ON dd.full_date = DATE(o.order_approved_at)\n",
    "JOIN dwh.dim_time dt ON\n",
    "    EXTRACT(HOUR FROM o.order_approved_at)::INT = dt.hours AND\n",
    "    EXTRACT(MINUTE FROM o.order_approved_at)::INT = dt.minutes AND\n",
    "    EXTRACT(SECOND FROM o.order_approved_at)::INT = dt.seconds\n",
    "\n",
    "JOIN dwh.dim_customers dc ON dc.customer_id = o.customer_id AND dc.is_current = TRUE\n",
    "JOIN dwh.dim_sellers ds ON ds.seller_id = oi.seller_id AND ds.is_current = TRUE\n",
    "JOIN dwh.dim_products dp ON dp.product_id = oi.product_id AND dp.is_current = TRUE\n",
    "JOIN dwh.dim_product_category dpc ON dpc.product_category_name = dp.product_category_name;\n",
    "\n",
    "\n",
    "-- Example ELT script for populating periodical_snapshot_order_trends\n",
    "INSERT INTO dwh.daily_periodical_snapshot_order_trends (\n",
    "    order_date,\n",
    "    year_day,\n",
    "    year_week,\n",
    "    year_month,\n",
    "    year_quarter,\n",
    "    year_semester,\n",
    "    distinct_customer_count,\n",
    "    distinct_seller_count,\n",
    "    order_id_count,\n",
    "    product_id_count,\n",
    "    sum_price,\n",
    "    sum_freight_value\n",
    ")\n",
    "SELECT\n",
    "    dd.full_date AS order_date,\n",
    "    dd.year_day,\n",
    "    dd.year_week,\n",
    "    dd.year_month,\n",
    "    dd.year_quarter,\n",
    "    dd.year_semester,\n",
    "\n",
    "    COUNT(DISTINCT o.customer_id) AS distinct_customer_count,\n",
    "    COUNT(DISTINCT oi.seller_id) AS distinct_seller_count,\n",
    "    COUNT(DISTINCT o.order_id) AS order_id_count,\n",
    "    COUNT(DISTINCT oi.product_id) AS product_id_count,\n",
    "    SUM(oi.price) AS sum_price,\n",
    "    SUM(oi.freight_value) AS sum_freight_value\n",
    "\n",
    "FROM stg.orders o\n",
    "JOIN stg.order_items oi ON oi.order_id = o.order_id\n",
    "JOIN dwh.dim_date dd ON dd.full_date = TO_DATE(o.order_approved_at, 'YYYY-MM-DD')\n",
    "\n",
    "-- Optional: avoid duplicate snapshot inserts\n",
    "LEFT JOIN dwh.daily_periodical_snapshot_order_trends existing\n",
    "    ON existing.order_date = dd.full_date\n",
    "WHERE o.order_approved_at IS NOT NULL\n",
    "  AND existing.order_date IS NULL\n",
    "\n",
    "GROUP BY\n",
    "    dd.full_date,\n",
    "    dd.year_day,\n",
    "    dd.year_week,\n",
    "    dd.year_month,\n",
    "    dd.year_quarter,\n",
    "    dd.year_semester;\n",
    "\n",
    "\n",
    " \n",
    "-- fact accumulating snapshot\n",
    "INSERT INTO dwh.accumulating_snapshot_logistic_performance (\n",
    "    order_id,\n",
    "    product_sk,\n",
    "    seller_sk,\n",
    "    customer_sk,\n",
    "    order_purchase_timestamp,\n",
    "    order_approved_at,\n",
    "    purchase_approve_difference,\n",
    "    order_estimated_delivery_date,\n",
    "    order_shipping_limit_date,\n",
    "    order_delivered_carrier_date,\n",
    "    order_delivered_customer_date,\n",
    "    approved_carrier_difference,\n",
    "    approved_delivered_difference,\n",
    "    order_purchase_date_id,\n",
    "    order_delivered_date_id,\n",
    "    last_updated,\n",
    "    order_status_code\n",
    ")\n",
    "SELECT\n",
    "    o.order_id::UUID,\n",
    "\n",
    "    dp.product_sk,\n",
    "    ds.seller_sk,\n",
    "    dc.customer_sk,\n",
    "\n",
    "    TO_TIMESTAMP(o.order_purchase_timestamp, 'YYYY-MM-DD HH24:MI:SS') AS order_purchase_ts,\n",
    "    TO_TIMESTAMP(o.order_approved_at, 'YYYY-MM-DD HH24:MI:SS') AS approved_ts,\n",
    "\n",
    "    CASE \n",
    "        WHEN o.order_approved_at IS NOT NULL \n",
    "        THEN TO_TIMESTAMP(o.order_approved_at, 'YYYY-MM-DD HH24:MI:SS') - TO_TIMESTAMP(o.order_purchase_timestamp, 'YYYY-MM-DD HH24:MI:SS')\n",
    "        ELSE NULL\n",
    "    END AS purchase_approve_diff,\n",
    "\n",
    "    TO_TIMESTAMP(o.order_estimated_delivery_date, 'YYYY-MM-DD') AS estimated_delivery,\n",
    "    TO_TIMESTAMP(oi.shipping_limit_date, 'YYYY-MM-DD HH24:MI:SS') AS shipping_limit,\n",
    "    TO_TIMESTAMP(o.order_delivered_carrier_date, 'YYYY-MM-DD HH24:MI:SS') AS delivered_carrier,\n",
    "    TO_TIMESTAMP(o.order_delivered_customer_date, 'YYYY-MM-DD HH24:MI:SS') AS delivered_customer,\n",
    "\n",
    "    CASE \n",
    "        WHEN o.order_delivered_carrier_date IS NOT NULL AND o.order_approved_at IS NOT NULL\n",
    "        THEN TO_TIMESTAMP(o.order_delivered_carrier_date, 'YYYY-MM-DD HH24:MI:SS') - TO_TIMESTAMP(o.order_approved_at, 'YYYY-MM-DD HH24:MI:SS')\n",
    "        ELSE NULL\n",
    "    END AS approved_carrier_diff,\n",
    "\n",
    "    CASE \n",
    "        WHEN o.order_delivered_customer_date IS NOT NULL AND o.order_approved_at IS NOT NULL\n",
    "        THEN TO_TIMESTAMP(o.order_delivered_customer_date, 'YYYY-MM-DD HH24:MI:SS') - TO_TIMESTAMP(o.order_approved_at, 'YYYY-MM-DD HH24:MI:SS')\n",
    "        ELSE NULL\n",
    "    END AS approved_delivered_diff,\n",
    "\n",
    "    dd_order.date_id AS order_purchase_date_id,\n",
    "    dd_delivered.date_id AS order_delivered_date_id,\n",
    "\n",
    "    NOW() AS last_updated,\n",
    "\n",
    "    o.order_status AS order_status_code\n",
    "\n",
    "FROM stg.orders o\n",
    "JOIN stg.order_items oi ON o.order_id = oi.order_id\n",
    "\n",
    "-- Dimension lookups with SCD type 2 handling\n",
    "JOIN dwh.dim_customers dc ON o.customer_id = dc.customer_id AND dc.is_current = TRUE\n",
    "JOIN dwh.dim_sellers ds ON oi.seller_id = ds.seller_id AND ds.is_current = TRUE\n",
    "JOIN dwh.dim_products dp ON oi.product_id = dp.product_id\n",
    "\n",
    "-- Dates\n",
    "LEFT JOIN dwh.dim_date dd_order ON dd_order.full_date = TO_DATE(o.order_purchase_timestamp, 'YYYY-MM-DD')\n",
    "LEFT JOIN dwh.dim_date dd_delivered ON dd_delivered.full_date = TO_DATE(o.order_delivered_customer_date, 'YYYY-MM-DD')\n",
    "\n",
    "-- Only if order_purchase_timestamp is present\n",
    "WHERE o.order_purchase_timestamp IS NOT NULL;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d698425e",
   "metadata": {},
   "source": [
    "## **Step \\#4 \\- Orchestrate ELT with Luigi (20 points)**\n",
    "\n",
    "- Use Luigi to orchestrate the pipeline you created\n",
    "- set up scheduling (using cron if applicable)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d69cc4",
   "metadata": {},
   "source": [
    "#### **Extract Script**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5e4a3765",
   "metadata": {},
   "outputs": [],
   "source": [
    "import luigi\n",
    "from datetime import datetime\n",
    "import logging\n",
    "import time\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(r\"C:\\Users\\LENOVO THINKPAD\\Documents\\Github\\Pacmann---Data-Storage_-Warehouse--Mart---Lake\\mentoring 2\\olist-dwh-project\")\n",
    "\n",
    "from pipeline.utility.db_conn import db_connection\n",
    "from pipeline.utility.read_sql import read_sql_file\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Define DIR\n",
    "DIR_ROOT_PROJECT = os.getenv(\"DIR_ROOT_PROJECT\")\n",
    "DIR_TEMP_LOG = os.getenv(\"DIR_TEMP_LOG\")\n",
    "DIR_TEMP_DATA = os.getenv(\"DIR_TEMP_DATA\")\n",
    "DIR_EXTRACT_QUERY = os.getenv(\"DIR_EXTRACT_QUERY\")\n",
    "DIR_LOG = os.getenv(\"DIR_LOG\")\n",
    "\n",
    "class Extract(luigi.Task):\n",
    "    # Define tables to be extracted from db sources\n",
    "    tables_to_extract = ['public.geolocation', \n",
    "                        'public.product_category_name_translation', \n",
    "                        'public.sellers', \n",
    "                        'public.customers', \n",
    "                        'public.products', \n",
    "                        'public.orders',\n",
    "                        'public.order_items',\n",
    "                        'public.order_payments',\n",
    "                        'public.order_reviews']\n",
    "    \n",
    "    def requires(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "    def run(self):\n",
    "        try:\n",
    "            # Configure logging\n",
    "            logging.basicConfig(filename = f'{DIR_TEMP_LOG}/logs.log', \n",
    "                                level = logging.INFO, \n",
    "                                format = '%(asctime)s - %(levelname)s - %(message)s')\n",
    "            \n",
    "            # Define db connection engine\n",
    "            src_engine, _ = db_connection()\n",
    "            \n",
    "            # Define the query using the SQL content\n",
    "            extract_query = read_sql_file(\n",
    "                file_path = f'{DIR_EXTRACT_QUERY}/extract-all-tables.sql'\n",
    "            )\n",
    "            \n",
    "            start_time = time.time()  # Record start time\n",
    "            logging.info(\"==================================STARTING EXTRACT DATA=======================================\")\n",
    "            \n",
    "            for index, table_name in enumerate(self.tables_to_extract):\n",
    "                try:\n",
    "                    # Read data into DataFrame\n",
    "                    df = pd.read_sql_query(extract_query.format(table_name = table_name), src_engine)\n",
    "\n",
    "                    # Write DataFrame to CSV\n",
    "                    df.to_csv(f\"{DIR_TEMP_DATA}/{table_name}.csv\", index=False)\n",
    "                    \n",
    "                    logging.info(f\"EXTRACT '{table_name}' - SUCCESS.\")\n",
    "                    \n",
    "                except Exception:\n",
    "                    logging.error(f\"EXTRACT '{table_name}' - FAILED.\")  \n",
    "                    raise Exception(f\"Failed to extract '{table_name}' tables\")\n",
    "            \n",
    "            logging.info(f\"Extract All Tables From Sources - SUCCESS\")\n",
    "            \n",
    "            end_time = time.time()  # Record end time\n",
    "            execution_time = end_time - start_time  # Calculate execution time\n",
    "            \n",
    "            # Get summary\n",
    "            summary_data = {\n",
    "                'timestamp': [datetime.now()],\n",
    "                'task': ['Extract'],\n",
    "                'status' : ['Success'],\n",
    "                'execution_time': [execution_time]\n",
    "            }\n",
    "            \n",
    "            # Get summary dataframes\n",
    "            summary = pd.DataFrame(summary_data)\n",
    "            \n",
    "            # Write DataFrame to CSV\n",
    "            summary.to_csv(f\"{DIR_TEMP_DATA}/extract-summary.csv\", index = False)\n",
    "                    \n",
    "        except Exception:   \n",
    "            logging.info(f\"Extract All Tables From Sources - FAILED\")\n",
    "             \n",
    "            # Get summary\n",
    "            summary_data = {\n",
    "                'timestamp': [datetime.now()],\n",
    "                'task': ['Extract'],\n",
    "                'status' : ['Failed'],\n",
    "                'execution_time': [0]\n",
    "            }\n",
    "            \n",
    "            # Get summary dataframes\n",
    "            summary = pd.DataFrame(summary_data)\n",
    "            \n",
    "            # Write DataFrame to CSV\n",
    "            summary.to_csv(f\"{DIR_TEMP_DATA}/extract-summary.csv\", index = False)\n",
    "            \n",
    "            # Write exception\n",
    "            raise Exception(f\"FAILED to execute EXTRACT TASK !!!\")\n",
    "        \n",
    "        logging.info(\"==================================ENDING EXTRACT DATA=======================================\")\n",
    "                \n",
    "    def output(self):\n",
    "        outputs = []\n",
    "        for table_name in self.tables_to_extract:\n",
    "            outputs.append(luigi.LocalTarget(f'{DIR_TEMP_DATA}/{table_name}.csv'))\n",
    "            \n",
    "        outputs.append(luigi.LocalTarget(f'{DIR_TEMP_DATA}/extract-summary.csv'))\n",
    "            \n",
    "        outputs.append(luigi.LocalTarget(f'{DIR_TEMP_LOG}/logs.log'))\n",
    "        return outputs\n",
    "    \n",
    "#if __name__ == '__main__':\n",
    " #   luigi.build([Extract()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5de0139f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG: Checking if Extract() is complete\n",
      "INFO: Informed scheduler that task   Extract__99914b932b   has status   PENDING\n",
      "INFO: Done scheduling tasks\n",
      "INFO: Running Worker with 1 processes\n",
      "DEBUG: Asking scheduler for work...\n",
      "DEBUG: Pending tasks: 1\n",
      "INFO: [pid 21524] Worker Worker(salt=6265168950, workers=1, host=LAPTOP-H2OPKO7T, username=LENOVO THINKPAD, pid=21524) running   Extract()\n",
      "INFO: [pid 21524] Worker Worker(salt=6265168950, workers=1, host=LAPTOP-H2OPKO7T, username=LENOVO THINKPAD, pid=21524) done      Extract()\n",
      "DEBUG: 1 running tasks, waiting for next task to finish\n",
      "INFO: Informed scheduler that task   Extract__99914b932b   has status   DONE\n",
      "DEBUG: Asking scheduler for work...\n",
      "DEBUG: Done\n",
      "DEBUG: There are no more tasks to run at this time\n",
      "INFO: Worker Worker(salt=6265168950, workers=1, host=LAPTOP-H2OPKO7T, username=LENOVO THINKPAD, pid=21524) was stopped. Shutting down Keep-Alive thread\n",
      "INFO: \n",
      "===== Luigi Execution Summary =====\n",
      "\n",
      "Scheduled 1 tasks of which:\n",
      "* 1 ran successfully:\n",
      "    - 1 Extract()\n",
      "\n",
      "This progress looks :) because there were no failed tasks or missing dependencies\n",
      "\n",
      "===== Luigi Execution Summary =====\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "luigi.build([Extract()], local_scheduler=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2217bd7d",
   "metadata": {},
   "source": [
    "#### **Load script**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b6fe4049",
   "metadata": {},
   "outputs": [],
   "source": [
    "import luigi\n",
    "import logging\n",
    "import pandas as pd\n",
    "import time\n",
    "import sqlalchemy\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(r\"C:\\Users\\LENOVO THINKPAD\\Documents\\Github\\Pacmann---Data-Storage_-Warehouse--Mart---Lake\\mentoring 2\\olist-dwh-project\")\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "from pipeline.extract import Extract\n",
    "from pipeline.utility.db_conn import db_connection\n",
    "from pipeline.utility.read_sql import read_sql_file\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Define DIR\n",
    "DIR_ROOT_PROJECT = os.getenv(\"DIR_ROOT_PROJECT\")\n",
    "DIR_TEMP_LOG = os.getenv(\"DIR_TEMP_LOG\")\n",
    "DIR_TEMP_DATA = os.getenv(\"DIR_TEMP_DATA\")\n",
    "DIR_LOAD_QUERY = os.getenv(\"DIR_LOAD_QUERY\")\n",
    "DIR_LOG = os.getenv(\"DIR_LOG\")\n",
    "\n",
    "# read sql query files to load from `public` to `stg` (only read, not execute)\n",
    "# reading csv file into df\n",
    "# establishing connection to dwh_engine\n",
    "# truncate / delete the content of the public schema\n",
    "# do `to_sql()` from the df, to the `public` schema\n",
    "# execute the queries to load from `public` to `stg` schema\n",
    "\n",
    "class Load(luigi.Task):   \n",
    "    \n",
    "    def requires(self):\n",
    "        return Extract()\n",
    "    \n",
    "    def run(self):\n",
    "         \n",
    "        # Configure logging\n",
    "        logging.basicConfig(filename = f'{DIR_TEMP_LOG}/logs.log', \n",
    "                            level = logging.INFO, \n",
    "                            format = '%(asctime)s - %(levelname)s - %(message)s')\n",
    "        \n",
    "        #----------------------------------------------------------------------------------------------------------------------------------------\n",
    "        # Read query to be executed\n",
    "        try:\n",
    "            # Read query to truncate public schema in dwh\n",
    "            truncate_query = read_sql_file(\n",
    "                file_path = f'{DIR_LOAD_QUERY}/load-public-truncate_tables.sql'\n",
    "            )\n",
    "            \n",
    "            # Read load query to staging schema\n",
    "            geolocation_query = read_sql_file(\n",
    "                file_path = f'{DIR_LOAD_QUERY}/load-stg-geolocation.sql'\n",
    "            )\n",
    "            \n",
    "            product_category_query = read_sql_file(\n",
    "                file_path = f'{DIR_LOAD_QUERY}/load-stg-product_category_name_translation.sql'\n",
    "            )\n",
    "            \n",
    "            sellers_query = read_sql_file(\n",
    "                file_path = f'{DIR_LOAD_QUERY}/load-stg-sellers.sql'\n",
    "            )\n",
    "            \n",
    "            customers_query = read_sql_file(\n",
    "                file_path = f'{DIR_LOAD_QUERY}/load-stg-customers.sql'\n",
    "            )\n",
    "            \n",
    "            products_query = read_sql_file(\n",
    "                file_path = f'{DIR_LOAD_QUERY}/load-stg-products.sql'\n",
    "            )\n",
    "            \n",
    "            orders_query = read_sql_file(\n",
    "                file_path = f'{DIR_LOAD_QUERY}/load-stg-orders.sql'\n",
    "            )  \n",
    "            \n",
    "            order_items_query = read_sql_file(\n",
    "                file_path = f'{DIR_LOAD_QUERY}/load-stg-order_items.sql'\n",
    "            )  \n",
    "            \n",
    "            order_payments_query = read_sql_file(\n",
    "                file_path = f'{DIR_LOAD_QUERY}/load-stg-order_payments.sql'\n",
    "            )\n",
    "            \n",
    "            order_reviews_query = read_sql_file(\n",
    "                file_path = f'{DIR_LOAD_QUERY}/load-stg-order_reviews.sql'\n",
    "            )                                        \n",
    "            \n",
    "            logging.info(\"Read Load Query - SUCCESS\")\n",
    "            \n",
    "        except Exception:\n",
    "            logging.error(\"Read Load Query - FAILED\")\n",
    "            raise Exception(\"Failed to read Load Query\")\n",
    "\n",
    "        #----------------------------------------------------------------------------------------------------------------------------------------\n",
    "        # Read Data to be load\n",
    "        try:\n",
    "            # Read csv\n",
    "            geolocation = pd.read_csv(self.input()[0].path)\n",
    "            product_category = pd.read_csv(self.input()[1].path)\n",
    "            sellers = pd.read_csv(self.input()[2].path)\n",
    "            customers = pd.read_csv(self.input()[3].path)\n",
    "            products = pd.read_csv(self.input()[4].path)\n",
    "            orders = pd.read_csv(self.input()[5].path)\n",
    "            order_items = pd.read_csv(self.input()[6].path)\n",
    "            order_payments = pd.read_csv(self.input()[7].path)\n",
    "            order_reviews = pd.read_csv(self.input()[8].path)\n",
    "            \n",
    "            logging.info(f\"Read Extracted Data - SUCCESS\")\n",
    "            \n",
    "        except Exception:\n",
    "            logging.error(f\"Read Extracted Data  - FAILED\")\n",
    "            raise Exception(\"Failed to Read Extracted Data\")\n",
    "        \n",
    "        \n",
    "        #----------------------------------------------------------------------------------------------------------------------------------------\n",
    "        # Establish connections to DWH\n",
    "        try:\n",
    "            _, dwh_engine = db_connection()\n",
    "            logging.info(f\"Connect to olist-dwh - SUCCESS\")\n",
    "            \n",
    "        except Exception:\n",
    "            logging.info(f\"Connect to olist-dwh - FAILED\")\n",
    "            raise Exception(\"Failed to connect to Data Warehouse\")\n",
    "        \n",
    "        \n",
    "        #----------------------------------------------------------------------------------------------------------------------------------------\n",
    "        # Truncate all tables in public schema before load\n",
    "        # This purpose to avoid errors because duplicate key value violates unique constraint\n",
    "        try:            \n",
    "            # Split the SQL queries if multiple queries are present\n",
    "            truncate_query = truncate_query.split(';')\n",
    "\n",
    "            # Remove newline characters and leading/trailing whitespaces\n",
    "            truncate_query = [query.strip() for query in truncate_query if query.strip()]\n",
    "            \n",
    "            # Create session\n",
    "            Session = sessionmaker(bind = dwh_engine)\n",
    "            session = Session()\n",
    "\n",
    "            # Execute each query\n",
    "            for query in truncate_query:\n",
    "                query = sqlalchemy.text(query)\n",
    "                session.execute(query)\n",
    "                \n",
    "            session.commit()\n",
    "            \n",
    "            # Close session\n",
    "            session.close()\n",
    "\n",
    "            logging.info(f\"Truncate `public` Schema in olist-dwh - SUCCESS\")\n",
    "        \n",
    "        except Exception:\n",
    "            logging.error(f\"Truncate `public` Schema in olist-dwh - FAILED\")\n",
    "            \n",
    "            raise Exception(\"Failed to Truncate `public` schema in olist-dwh\")\n",
    "        \n",
    "        \n",
    "        \n",
    "        #----------------------------------------------------------------------------------------------------------------------------------------\n",
    "        # Record start time for loading tables\n",
    "        start_time = time.time()  \n",
    "        logging.info(\"==================================STARTING LOAD DATA=======================================\")\n",
    "        # Load to tables\n",
    "        try:\n",
    "            \n",
    "            try:\n",
    "                # Load to public schema   \n",
    "                geolocation.to_sql('geolocation', \n",
    "                                    con = dwh_engine, \n",
    "                                    if_exists = 'append', \n",
    "                                    index = False, \n",
    "                                    schema = 'public')\n",
    "                \n",
    "                product_category.to_sql('product_category_name_translation', \n",
    "                                    con = dwh_engine, \n",
    "                                    if_exists = 'append', \n",
    "                                    index = False, \n",
    "                                    schema = 'public')\n",
    "                \n",
    "                sellers.to_sql('sellers', \n",
    "                                con = dwh_engine, \n",
    "                                if_exists = 'append', \n",
    "                                index = False, \n",
    "                                schema = 'public')\n",
    "                \n",
    "                customers.to_sql('customers', \n",
    "                            con = dwh_engine, \n",
    "                            if_exists = 'append', \n",
    "                            index = False, \n",
    "                            schema = 'public')\n",
    "                \n",
    "                products.to_sql('products', \n",
    "                            con = dwh_engine, \n",
    "                            if_exists = 'append', \n",
    "                            index = False, \n",
    "                            schema = 'public')\n",
    "                \n",
    "                orders.to_sql('orders', \n",
    "                            con = dwh_engine, \n",
    "                            if_exists = 'append', \n",
    "                            index = False, \n",
    "                            schema = 'public')\n",
    "                \n",
    "                order_items.to_sql('order_items', \n",
    "                                   con = dwh_engine, \n",
    "                                   if_exists = 'append', \n",
    "                                   index = False, \n",
    "                                   schema= 'public')\n",
    "                \n",
    "                order_payments.to_sql('order_payments',\n",
    "                                      con = dwh_engine,\n",
    "                                      if_exists= 'append',\n",
    "                                      index=False,\n",
    "                                      schema='public')\n",
    "                \n",
    "                order_reviews.to_sql('order_reviews',\n",
    "                                     con = dwh_engine,\n",
    "                                     if_exists= 'append',\n",
    "                                     index=False,\n",
    "                                     schema='public')\n",
    "                \n",
    "                logging.info(f\"LOAD All Tables To olist-dwh-public - SUCCESS\")\n",
    "                \n",
    "            except Exception:\n",
    "                logging.error(f\"LOAD All Tables To olist-dwh-public - FAILED\")\n",
    "                raise Exception('Failed Load Tables To olist-dwh-public')\n",
    "            \n",
    "            \n",
    "            #----------------------------------------------------------------------------------------------------------------------------------------\n",
    "            # Load from public schema to staging schema\n",
    "            try:\n",
    "                # List query\n",
    "                load_stg_queries = [geolocation_query, product_category_query,\n",
    "                                    sellers_query, customers_query, products_query,\n",
    "                                    orders_query, order_items_query, order_payments_query, order_reviews_query]\n",
    "                \n",
    "                # Create session\n",
    "                Session = sessionmaker(bind = dwh_engine)\n",
    "                session = Session()\n",
    "\n",
    "                # Execute each query\n",
    "                for query in load_stg_queries:\n",
    "                    query = sqlalchemy.text(query)\n",
    "                    session.execute(query)\n",
    "                    \n",
    "                session.commit()\n",
    "                \n",
    "                # Close session\n",
    "                session.close()\n",
    "                \n",
    "                logging.info(\"LOAD All Tables To DWH-Staging - SUCCESS\")\n",
    "                \n",
    "            except Exception:\n",
    "                logging.error(\"LOAD All Tables To DWH-Staging - FAILED\")\n",
    "                raise Exception('Failed Load Tables To DWH-Staging')\n",
    "        \n",
    "        \n",
    "            # Record end time for loading tables\n",
    "            end_time = time.time()  \n",
    "            execution_time = end_time - start_time  # Calculate execution time\n",
    "            \n",
    "            # Get summary\n",
    "            summary_data = {\n",
    "                'timestamp': [datetime.now()],\n",
    "                'task': ['Load'],\n",
    "                'status' : ['Success'],\n",
    "                'execution_time': [execution_time]\n",
    "            }\n",
    "\n",
    "            # Get summary dataframes\n",
    "            summary = pd.DataFrame(summary_data)\n",
    "            \n",
    "            # Write Summary to CSV\n",
    "            summary.to_csv(f\"{DIR_TEMP_DATA}/load-summary.csv\", index = False)\n",
    "            \n",
    "                        \n",
    "        #----------------------------------------------------------------------------------------------------------------------------------------\n",
    "        except Exception:\n",
    "            # Get summary\n",
    "            summary_data = {\n",
    "                'timestamp': [datetime.now()],\n",
    "                'task': ['Load'],\n",
    "                'status' : ['Failed'],\n",
    "                'execution_time': [0]\n",
    "            }\n",
    "\n",
    "            # Get summary dataframes\n",
    "            summary = pd.DataFrame(summary_data)\n",
    "            \n",
    "            # Write Summary to CSV\n",
    "            summary.to_csv(f\"{DIR_TEMP_DATA}/load-summary.csv\", index = False)\n",
    "            \n",
    "            logging.error(\"LOAD All Tables To DWH - FAILED\")\n",
    "            raise Exception('Failed Load Tables To DWH')   \n",
    "        \n",
    "        logging.info(\"==================================ENDING LOAD DATA=======================================\")\n",
    "        \n",
    "    #----------------------------------------------------------------------------------------------------------------------------------------\n",
    "    def output(self):\n",
    "        return [luigi.LocalTarget(f'{DIR_TEMP_LOG}/logs.log'),\n",
    "                luigi.LocalTarget(f'{DIR_TEMP_DATA}/load-summary.csv')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1ea469c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG: Checking if Extract() is complete\n",
      "INFO: Informed scheduler that task   Extract__99914b932b   has status   DONE\n",
      "DEBUG: Checking if Load() is complete\n",
      "DEBUG: Checking if Extract() is complete\n",
      "INFO: Informed scheduler that task   Load__99914b932b   has status   PENDING\n",
      "INFO: Informed scheduler that task   Extract__99914b932b   has status   DONE\n",
      "INFO: Done scheduling tasks\n",
      "INFO: Running Worker with 1 processes\n",
      "DEBUG: Asking scheduler for work...\n",
      "DEBUG: Pending tasks: 1\n",
      "INFO: [pid 21524] Worker Worker(salt=4202338360, workers=1, host=LAPTOP-H2OPKO7T, username=LENOVO THINKPAD, pid=21524) running   Load()\n",
      "INFO: [pid 21524] Worker Worker(salt=4202338360, workers=1, host=LAPTOP-H2OPKO7T, username=LENOVO THINKPAD, pid=21524) done      Load()\n",
      "DEBUG: 1 running tasks, waiting for next task to finish\n",
      "INFO: Informed scheduler that task   Load__99914b932b   has status   DONE\n",
      "DEBUG: Asking scheduler for work...\n",
      "DEBUG: Done\n",
      "DEBUG: There are no more tasks to run at this time\n",
      "INFO: Worker Worker(salt=4202338360, workers=1, host=LAPTOP-H2OPKO7T, username=LENOVO THINKPAD, pid=21524) was stopped. Shutting down Keep-Alive thread\n",
      "INFO: \n",
      "===== Luigi Execution Summary =====\n",
      "\n",
      "Scheduled 2 tasks of which:\n",
      "* 1 complete ones were encountered:\n",
      "    - 1 Extract()\n",
      "* 1 ran successfully:\n",
      "    - 1 Load()\n",
      "\n",
      "This progress looks :) because there were no failed tasks or missing dependencies\n",
      "\n",
      "===== Luigi Execution Summary =====\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "luigi.build([Extract(),Load()], local_scheduler=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb7b9d58",
   "metadata": {},
   "source": [
    "#### **Transform Script**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1747c82a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import luigi\n",
    "import logging\n",
    "import pandas as pd\n",
    "import time\n",
    "import sqlalchemy\n",
    "from datetime import datetime\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(r\"C:\\Users\\LENOVO THINKPAD\\Documents\\Github\\Pacmann---Data-Storage_-Warehouse--Mart---Lake\\mentoring 2\\olist-dwh-project\")\n",
    "\n",
    "from pipeline.extract import Extract\n",
    "from pipeline.load import Load\n",
    "from pipeline.utility.db_conn import db_connection\n",
    "from pipeline.utility.read_sql import read_sql_file\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Define DIR\n",
    "DIR_ROOT_PROJECT = os.getenv(\"DIR_ROOT_PROJECT\")\n",
    "DIR_TEMP_LOG = os.getenv(\"DIR_TEMP_LOG\")\n",
    "DIR_TEMP_DATA = os.getenv(\"DIR_TEMP_DATA\")\n",
    "DIR_TRANSFORM_QUERY = os.getenv(\"DIR_TRANSFORM_QUERY\")\n",
    "DIR_LOG = os.getenv(\"DIR_LOG\")\n",
    "\n",
    "class Transform(luigi.Task):\n",
    "    \n",
    "    def requires(self):\n",
    "        return Load()\n",
    "    \n",
    "    def run(self):\n",
    "         \n",
    "        # Configure logging\n",
    "        logging.basicConfig(filename = f'{DIR_TEMP_LOG}/logs.log', \n",
    "                            level = logging.INFO, \n",
    "                            format = '%(asctime)s - %(levelname)s - %(message)s')\n",
    "        \n",
    "        #----------------------------------------------------------------------------------------------------------------------------------------\n",
    "        # Establish connections to DWH\n",
    "        try:\n",
    "            _, dwh_engine = db_connection()\n",
    "            logging.info(f\"Connect to DWH - SUCCESS\")\n",
    "            \n",
    "        except Exception:\n",
    "            logging.info(f\"Connect to DWH - FAILED\")\n",
    "            raise Exception(\"Failed to connect to Data Warehouse\")\n",
    "        \n",
    "        #----------------------------------------------------------------------------------------------------------------------------------------\n",
    "        # Read query to be executed\n",
    "        try:\n",
    "            \n",
    "            # Read transform query to final schema\n",
    "            dim_geolocation_query = read_sql_file(\n",
    "                file_path = f'{DIR_TRANSFORM_QUERY}/transform-dwh-dim_geolocation.sql'\n",
    "            )\n",
    "            \n",
    "            dim_product_category_query = read_sql_file(\n",
    "                file_path = f'{DIR_TRANSFORM_QUERY}/transform-dwh-dim_product_category.sql'\n",
    "            )\n",
    "            \n",
    "            dim_sellers_query = read_sql_file(\n",
    "                file_path = f'{DIR_TRANSFORM_QUERY}/transform-dwh-dim_sellers.sql'\n",
    "            )\n",
    "            \n",
    "            dim_customers_query = read_sql_file(\n",
    "                file_path = f'{DIR_TRANSFORM_QUERY}/transform-dwh-dim_customers.sql'\n",
    "            )\n",
    "            \n",
    "            dim_products_query = read_sql_file(\n",
    "                file_path = f'{DIR_TRANSFORM_QUERY}/transform-dwh-dim_products.sql'\n",
    "            )\n",
    "            \n",
    "            fact_orders_comprehensive_query = read_sql_file(\n",
    "                file_path = f'{DIR_TRANSFORM_QUERY}/transform-dwh-fact_orders_comprehensive.sql'\n",
    "            )\n",
    "            \n",
    "            fact_reviews_sentiment_query = read_sql_file(\n",
    "                file_path = f'{DIR_TRANSFORM_QUERY}/transform-dwh-fact_reviews_sentiment.sql'\n",
    "            )\n",
    "            \n",
    "            fact_daily_periodicals_query = read_sql_file(\n",
    "                file_path = f'{DIR_TRANSFORM_QUERY}/transform-dwh-fact_daily_periodical_snapshot_order_trends.sql'\n",
    "            )\n",
    "            \n",
    "            fact_accumulating_snapshot_query = read_sql_file(\n",
    "                file_path = f'{DIR_TRANSFORM_QUERY}/transform-dwh-fact_accumulating_snapshot_logistic_performance.sql'\n",
    "            )\n",
    "            \n",
    "            logging.info(\"Read Transform Query - SUCCESS\")\n",
    "            \n",
    "        except Exception:\n",
    "            logging.error(\"Read Transform Query - FAILED\")\n",
    "            raise Exception(\"Failed to read Transform Query\")        \n",
    "        \n",
    "        #----------------------------------------------------------------------------------------------------------------------------------------\n",
    "        # Record start time for transform tables\n",
    "        start_time = time.time()\n",
    "        logging.info(\"==================================STARTING TRANSFROM DATA=======================================\")  \n",
    "               \n",
    "        # Transform to dimensions tables\n",
    "        try:\n",
    "            # Create session\n",
    "            Session = sessionmaker(bind = dwh_engine)\n",
    "            session = Session()\n",
    "            \n",
    "            # Transform to `dwh` tables\n",
    "            \n",
    "            query = sqlalchemy.text(dim_geolocation_query)\n",
    "            session.execute(query)\n",
    "            logging.info(\"Transform to 'dwh.dim_geolocation' - SUCCESS\")\n",
    "            \n",
    "            query = sqlalchemy.text(dim_product_category_query)\n",
    "            session.execute(query)\n",
    "            logging.info(\"Transform to 'dwh.dim_product_category' - SUCCESS\")\n",
    "            \n",
    "            query = sqlalchemy.text(dim_sellers_query)\n",
    "            session.execute(query)\n",
    "            logging.info(\"Transform to 'dwh.dim_sellers' - SUCCESS\")\n",
    "            \n",
    "            query = sqlalchemy.text(dim_customers_query)\n",
    "            session.execute(query)\n",
    "            logging.info(\"Transform to 'dwh.dim_customers' - SUCCESS\")\n",
    "        \n",
    "            query = sqlalchemy.text(dim_products_query)\n",
    "            session.execute(query)\n",
    "            logging.info(\"Transform to 'dwh.dim_products' - SUCCESS\")\n",
    "            \n",
    "            query = sqlalchemy.text(fact_orders_comprehensive_query)\n",
    "            session.execute(query)\n",
    "            logging.info(\"Transform to 'dwh.fact_orders_comprehensive' - SUCCESS\")\n",
    "            \n",
    "            query = sqlalchemy.text(fact_reviews_sentiment_query)\n",
    "            session.execute(query)\n",
    "            logging.info(\"Transform to 'dwh.fact_reviews_sentiment' - SUCCESS\")\n",
    "            \n",
    "            query = sqlalchemy.text(fact_daily_periodicals_query)\n",
    "            session.execute(query)\n",
    "            logging.info(\"Transform to 'dwh.fact_daily_periodicals_order_trends' - SUCCESS\")\n",
    "            \n",
    "            query = sqlalchemy.text(fact_accumulating_snapshot_query)\n",
    "            session.execute(query)\n",
    "            logging.info(\"Transform to 'dwh.fact_accumulating_snapshot_logistic_performance' - SUCCESS\")\n",
    "            \n",
    "            # Commit transaction\n",
    "            session.commit()\n",
    "            \n",
    "            # Close session\n",
    "            session.close()\n",
    "\n",
    "            logging.info(f\"Transform to All Dimensions and Fact Tables - SUCCESS\")\n",
    "            \n",
    "            # Record end time for loading tables\n",
    "            end_time = time.time()  \n",
    "            execution_time = end_time - start_time  # Calculate execution time\n",
    "            \n",
    "            # Get summary\n",
    "            summary_data = {\n",
    "                'timestamp': [datetime.now()],\n",
    "                'task': ['Transform'],\n",
    "                'status' : ['Success'],\n",
    "                'execution_time': [execution_time]\n",
    "            }\n",
    "\n",
    "            # Get summary dataframes\n",
    "            summary = pd.DataFrame(summary_data)\n",
    "            \n",
    "            # Write Summary to CSV\n",
    "            summary.to_csv(f\"{DIR_TEMP_DATA}/transform-summary.csv\", index = False)\n",
    "            \n",
    "        except Exception:\n",
    "            logging.error(f\"Transform to All Dimensions and Fact Tables - FAILED\")\n",
    "        \n",
    "            # Get summary\n",
    "            summary_data = {\n",
    "                'timestamp': [datetime.now()],\n",
    "                'task': ['Transform'],\n",
    "                'status' : ['Failed'],\n",
    "                'execution_time': [0]\n",
    "            }\n",
    "\n",
    "            # Get summary dataframes\n",
    "            summary = pd.DataFrame(summary_data)\n",
    "            \n",
    "            # Write Summary to CSV\n",
    "            summary.to_csv(f\"{DIR_TEMP_DATA}/transform-summary.csv\", index = False)\n",
    "            \n",
    "            logging.error(\"Transform Tables - FAILED\")\n",
    "            raise Exception('Failed Transforming Tables')   \n",
    "        \n",
    "        logging.info(\"==================================ENDING TRANSFROM DATA=======================================\") \n",
    "\n",
    "    #----------------------------------------------------------------------------------------------------------------------------------------\n",
    "    def output(self):\n",
    "        return [luigi.LocalTarget(f'{DIR_TEMP_LOG}/logs.log'),\n",
    "                luigi.LocalTarget(f'{DIR_TEMP_DATA}/transform-summary.csv')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d80a842c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG: Checking if Extract() is complete\n",
      "INFO: Informed scheduler that task   Extract__99914b932b   has status   DONE\n",
      "DEBUG: Checking if Load() is complete\n",
      "INFO: Informed scheduler that task   Load__99914b932b   has status   DONE\n",
      "DEBUG: Checking if Transform() is complete\n",
      "DEBUG: Checking if Load() is complete\n",
      "INFO: Informed scheduler that task   Transform__99914b932b   has status   PENDING\n",
      "INFO: Informed scheduler that task   Load__99914b932b   has status   DONE\n",
      "INFO: Done scheduling tasks\n",
      "INFO: Running Worker with 1 processes\n",
      "DEBUG: Asking scheduler for work...\n",
      "DEBUG: Pending tasks: 1\n",
      "INFO: [pid 21524] Worker Worker(salt=8244291368, workers=1, host=LAPTOP-H2OPKO7T, username=LENOVO THINKPAD, pid=21524) running   Transform()\n",
      "INFO: [pid 21524] Worker Worker(salt=8244291368, workers=1, host=LAPTOP-H2OPKO7T, username=LENOVO THINKPAD, pid=21524) done      Transform()\n",
      "DEBUG: 1 running tasks, waiting for next task to finish\n",
      "INFO: Informed scheduler that task   Transform__99914b932b   has status   DONE\n",
      "DEBUG: Asking scheduler for work...\n",
      "DEBUG: Done\n",
      "DEBUG: There are no more tasks to run at this time\n",
      "INFO: Worker Worker(salt=8244291368, workers=1, host=LAPTOP-H2OPKO7T, username=LENOVO THINKPAD, pid=21524) was stopped. Shutting down Keep-Alive thread\n",
      "INFO: \n",
      "===== Luigi Execution Summary =====\n",
      "\n",
      "Scheduled 3 tasks of which:\n",
      "* 2 complete ones were encountered:\n",
      "    - 1 Extract()\n",
      "    - 1 Load()\n",
      "* 1 ran successfully:\n",
      "    - 1 Transform()\n",
      "\n",
      "This progress looks :) because there were no failed tasks or missing dependencies\n",
      "\n",
      "===== Luigi Execution Summary =====\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "luigi.build([Extract(),Load(),Transform()], local_scheduler=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45321d21",
   "metadata": {},
   "source": [
    "### **`elt_main` script**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ce8c9cbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG: Checking if Extract() is complete\n",
      "INFO: Informed scheduler that task   Extract__99914b932b   has status   DONE\n",
      "DEBUG: Checking if Load() is complete\n",
      "INFO: Informed scheduler that task   Load__99914b932b   has status   DONE\n",
      "DEBUG: Checking if Transform() is complete\n",
      "INFO: Informed scheduler that task   Transform__99914b932b   has status   DONE\n",
      "INFO: Done scheduling tasks\n",
      "INFO: Running Worker with 1 processes\n",
      "DEBUG: Asking scheduler for work...\n",
      "DEBUG: Done\n",
      "DEBUG: There are no more tasks to run at this time\n",
      "INFO: Worker Worker(salt=559604840, workers=1, host=LAPTOP-H2OPKO7T, username=LENOVO THINKPAD, pid=21524) was stopped. Shutting down Keep-Alive thread\n",
      "INFO: \n",
      "===== Luigi Execution Summary =====\n",
      "\n",
      "Scheduled 3 tasks of which:\n",
      "* 3 complete ones were encountered:\n",
      "    - 1 Extract()\n",
      "    - 1 Load()\n",
      "    - 1 Transform()\n",
      "\n",
      "Did not run any tasks\n",
      "This progress looks :) because there were no failed tasks or missing dependencies\n",
      "\n",
      "===== Luigi Execution Summary =====\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred: [Errno 2] No such file or directory: 'C:\\\\Users\\\\LENOVO THINKPAD\\\\Documents\\\\Github\\\\Pacmann---Data-Storage_-Warehouse--Mart---Lake\\\\mentoring 2\\\\olist-dwh-project\\\\logs\\\\/logs.log'\n",
      "An error occurred: [WinError 32] The process cannot access the file because it is being used by another process: 'C:\\\\Users\\\\LENOVO THINKPAD\\\\Documents\\\\Github\\\\Pacmann---Data-Storage_-Warehouse--Mart---Lake\\\\mentoring 2\\\\olist-dwh-project\\\\pipeline\\\\temp\\\\log\\\\logs.log'\n"
     ]
    }
   ],
   "source": [
    "import luigi\n",
    "#import sentry_sdk\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "from pipeline.extract import Extract\n",
    "from pipeline.load import Load\n",
    "from pipeline.transform import Transform\n",
    "from pipeline.utility.concat_dataframe import concat_dataframes\n",
    "from pipeline.utility.copy_log import copy_log\n",
    "from pipeline.utility.delete_temp_data import delete_temp\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Read env variables\n",
    "DIR_ROOT_PROJECT = os.getenv(\"DIR_ROOT_PROJECT\")\n",
    "DIR_TEMP_LOG = os.getenv(\"DIR_TEMP_LOG\")\n",
    "DIR_TEMP_DATA = os.getenv(\"DIR_TEMP_DATA\")\n",
    "DIR_LOG = os.getenv(\"DIR_LOG\")\n",
    "SENTRY_DSN = os.getenv(\"SENTRY_DSN\")\n",
    "\n",
    "# Track the error using sentry\n",
    "#sentry_sdk.init(\n",
    "    #dsn = f\"{SENTRY_DSN}\"\n",
    "#)\n",
    "\n",
    "# Function to ensure a file exists with a specified header\n",
    "def ensure_file_exists_with_header(file_path, header=None):\n",
    "    if not os.path.exists(file_path):\n",
    "        with open(file_path, 'w') as f:\n",
    "            if header:\n",
    "                f.write(header)\n",
    "\n",
    "# Ensure pipeline_summary.csv exists with header\n",
    "pipeline_summary_path = f'{DIR_ROOT_PROJECT}/pipeline_summary.csv'\n",
    "ensure_file_exists_with_header(pipeline_summary_path, 'timestamp,task,status,execution_time\\n')\n",
    "\n",
    "# Execute the functions when the script is run\n",
    "#if __name__ == \"__main__\":\n",
    "    # Build the task\n",
    "luigi.build([Extract(),\n",
    "                Load(),\n",
    "                Transform()], local_scheduler=True)\n",
    "\n",
    "# Concat temp extract summary to final summary\n",
    "concat_dataframes(\n",
    "    df1 = pd.read_csv(f'{DIR_ROOT_PROJECT}/pipeline_summary.csv'),\n",
    "    df2 = pd.read_csv(f'{DIR_TEMP_DATA}/extract-summary.csv')\n",
    ")\n",
    "\n",
    "# Concat temp load summary to final summary\n",
    "concat_dataframes(\n",
    "    df1 = pd.read_csv(f'{DIR_ROOT_PROJECT}/pipeline_summary.csv'),\n",
    "    df2 = pd.read_csv(f'{DIR_TEMP_DATA}/load-summary.csv')\n",
    ")\n",
    "\n",
    "# Concat temp load summary to final summary\n",
    "concat_dataframes(\n",
    "    df1 = pd.read_csv(f'{DIR_ROOT_PROJECT}/pipeline_summary.csv'),\n",
    "    df2 = pd.read_csv(f'{DIR_TEMP_DATA}/transform-summary.csv')\n",
    ")\n",
    "\n",
    "# Append log from temp to final log\n",
    "copy_log(\n",
    "    source_file = f'{DIR_TEMP_LOG}/logs.log',\n",
    "    destination_file = f'{DIR_LOG}/logs.log'\n",
    ")\n",
    "\n",
    "# Delete temp data\n",
    "delete_temp(\n",
    "    directory = f'{DIR_TEMP_DATA}'\n",
    ")\n",
    "\n",
    "# Delete temp log\n",
    "delete_temp(\n",
    "    directory = f'{DIR_TEMP_LOG}'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae80c09b",
   "metadata": {},
   "source": [
    "## **Step \\#5 \\- Create Report(5 points)**\n",
    "\n",
    "The report can take the form of an article (which should include a link to your GitHub project) or a README file on GitHub. Ensure you thoroughly explain the points mentioned above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "579af5af",
   "metadata": {},
   "source": [
    "### **Data Pipeline Report**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ad4371",
   "metadata": {},
   "source": [
    "This project is an attempt to design a data warehouse and data pipeline for an e-commerce platform. Olist, the e-commerce in question, is a Brazilian venture, connecting various sellers and customers across the nation. Therefore, the need for a robust data warehouse and data pipeline design is of utmost importance. Here, we are trying to provide the solution for those two concerns.\n",
    "\n",
    "#### **Pretend Interview with Shareholders**\n",
    "\n",
    "##### 1. Which dimension tables are likely to have changes overtime that we should track of?\n",
    "  \n",
    "  **Response**\n",
    "  \n",
    "  Primarily `customers` and `sellers`. </br>\n",
    "  Customers may change their address or city (captured via `geolocation`) and sellers sometimes update their location or business names. </br>\n",
    "  `products` and `category name` are mostly static, but we may occassionally reclassify a product category\n",
    "  \n",
    "##### 2. Do we need to maintain historical versions of data?\n",
    "  \n",
    "  **Response**\n",
    "  \n",
    "  We wanted to preserve historical changes for both `customers` and `sellers`, especially location changes, as they can affect delivery time and logistics. </br>\n",
    "  For `products` and `category_name`, just keeping the latest version is enough since changes are rare and typically administrative </br>\n",
    "  \n",
    "##### 3. What kind of historical tracking is preferred, -- full versioning (type 2), overwrite (type 1), or current vs previous field (type 3)?\n",
    "  \n",
    "  **Response**\n",
    "  \n",
    "  SCD Type 2 (full-versioning)\n",
    "  \n",
    "  - `dim_customers`\n",
    "  - `dim_sellers`\n",
    "  \n",
    "  SCD Type 1 (overwrite)\n",
    "  \n",
    "  - `dim_category_name`\n",
    "  - `dim_products`\n",
    "\n",
    "##### 4. How should we handle geolocation changes? Should customers and sellers be linked to geolocation with historical traceability?\n",
    "  \n",
    "  **Response**\n",
    "  \n",
    "  Yes, because geolocation information, current or previous, is valuable for analysis, eg. comparing delivery times or satisfaction scores before and after a move. </br>\n",
    "  So we'll need foreign key-based Type 2 handling for geolocation information.\n",
    "  \n",
    "##### 5. Who will consume the historical data, and how will it be used (eg. in reports, machine learning, fraud detection, etc)?\n",
    "  \n",
    "  **Response**\n",
    "  \n",
    "  - Data analyst and operations team\n",
    "    - delivery performance reports, churn analysis, and predictive models\n",
    "\n",
    "#### **Determining SCD / Slowly Changing Dimension types for the tables**\n",
    "\n",
    "**SCD Type 1 / Overwrite**\n",
    "\n",
    "- `dim_product_category_name`\n",
    "    - track changes in the names of the category\n",
    "    - utilizing Type 1 / overwrite so as to simplify the overall process\n",
    "    - historical tracking of category names are not significant because at the end of the day, the main analysis to be conducted are mainly about products, customers, sellers, locations, and periodicals.\n",
    "\n",
    "**SCD Type 2 / Add New Row-Column**\n",
    "\n",
    "- `dim_customers`\n",
    "    - customers may change their location\n",
    "    - because of this change, logistic fees and performance changes may affect future purchases\n",
    "- `dim_sellers`\n",
    "    - sellers may change their location\n",
    "    - because of this change, \n",
    "- `dim_products`\n",
    "    - changes in products may reflect changes in purchases\n",
    "    - for example, the bigger the dimension may inflict higher freight cost, which may affect purchasing decision\n",
    "    - also, the changes in price tends to affect purchasing decision also, especially if the customers are price-sensitive\n",
    "    - hence, it is important to track price changes and dimension changes in relation to the total price & cost that the customers need to pay.\n",
    "\n",
    "#### **ELT / Extract, Load, Transform with Python and SQL, and Luigi tasks**\n",
    "\n",
    "The source of the database is in `olist-src` database, while the target database is in the `olist-dwh` database. In its current state, the `olist-dwh` database is empty without any schema or database. Hence, we need to design schemas and tables in the `olist-dwh` database.\n",
    "\n",
    "1. Design `public` schema in data warehouse (using the same DDL and schema as the tables in the `olist-src` database)\n",
    "\n",
    "2. Design `stg` / staging schema in `olist-dwh`(using similar schema to the `public` schema, but addressing the mistyped column names in the `public` schema)\n",
    "\n",
    "3. Design `dwh` / data warehouse schema in `olist-dwh`, with dimension and fact tables. The design already addresses the SCD types for the required tables, and comprehensive fact tables to address the need for data analytics (sales performance and logistic performance).\n",
    "\n",
    "4. Create an SQL script to fill the `dim.date` and `dim.time` tables in the `dwh` schema in `olist-dwh` database.\n",
    "\n",
    "5. Create a python script to extract the data from `olist-src` to csv files, named `extract.py`.\n",
    "\n",
    "6. Create SQL scripts to accomplish these functions:\n",
    "  - transfer data from `public` schema to `stg` schema in `olist-dwh`\n",
    "  - transfer data from `stg` schema to `dwh` schema in `olist-dwh`\n",
    "\n",
    "7. Create a python script named `load.py` to accomplish these functions\n",
    "  - read the csv files results from `extract.py` into dataframes\n",
    "  - read the sql scripts to transfer from `public` schema to `stg` schema\n",
    "  - clean the tables in `public` schema before being inserted new data\n",
    "  - put the data from the dataframe previously into the `public` schema\n",
    "  - transfer the data from the `public` schema to the `stg` schema\n",
    "\n",
    "8. Create a python script named `transform.py` to transfer the data from `stg` schema to `dwh` schema\n",
    "\n",
    "9. Combine all the scripts together inside `elt_main.py` to run all the scripts together."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_general",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
